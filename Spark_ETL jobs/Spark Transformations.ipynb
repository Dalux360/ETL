{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f7c138",
   "metadata": {},
   "source": [
    "# Common Spark Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3949ae",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32c331",
   "metadata": {},
   "source": [
    "For this notebook I'll be: \n",
    "- Reading data into Spark\n",
    "- Implementing projections and filters\n",
    "- Renaming and adding columns\n",
    "- Performing aggregations\n",
    "- Saving the work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54684e",
   "metadata": {},
   "source": [
    "To achieve the obectives set out in the above statement, I'll also be Using the Pyspark API, which forms a part of the Spark Core.\n",
    "\n",
    "Let's get started shall we :)\n",
    "\n",
    "It's always important to check the version of Spark you're currently working with because things change frequently and the this helps you feref to the correct documentation when the need arises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969e19fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca95ea",
   "metadata": {},
   "source": [
    "Awesome, now that Spark is installed, let me clarify that I'll be running it locally, therefore the `driver` and the `executor`  will be located on the same machine ie `localhost`.\n",
    "\n",
    "Ok, now in order to get Spark to execute my instructions through the PySpark API, I first need to register a `SparkContext` followed by setting up a `SparkSession.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb383a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/18 10:27:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initiate the Spark Context\n",
    "sc = SparkContext()\n",
    "#Initiate the Spark Session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9dc640",
   "metadata": {},
   "source": [
    "I'll be using Bitcoin dataset from kaggle for this analysis. I'll also be working with the spark dataframe as apposed to RDD in this example.\n",
    "\n",
    "We know that spark can infer the schema of a dataset that it reads (2 passes of the data are required ofr this) or we can define the schema ourselves when reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef29a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc_df = spark.read.csv('/Users/daluxmba/Desktop/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98fc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "| Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In spark we use the show() method to display the dataframe or to have a look at it's contents\n",
    "btc_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4baebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|summary|           Timestamp|   Open|   High|    Low|  Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|  count|             4857377|4857377|4857377|4857377|4857377|     4857377|          4857377|       4857377|\n",
      "|   mean|1.4713007665042922E9|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "| stddev| 8.428019437553181E7|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "|    min|          1325317920|     10|     10|    1.5|    1.5|           0|                0|            10|\n",
      "|    max|          1617148800|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcf014",
   "metadata": {},
   "source": [
    "Now lets take a look at the schema here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8918bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume_(BTC): string (nullable = true)\n",
      " |-- Volume_(Currency): string (nullable = true)\n",
      " |-- Weighted_Price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31d06",
   "metadata": {},
   "source": [
    "From the above I can see that spark infered all of the data types incorrectly and basically read everything as a string.\n",
    "- Timestamp should not be a string\n",
    "- Open, High, Low, Close, Voluems, and Weighted Price should all be numeric\n",
    "\n",
    "\n",
    "To better read in this data, I can manually specify the schema using built-in data types in Spark.\n",
    "- `StructType([])` defines the overall structure of the dataframe\n",
    "- `StructField()` defines the data type of each column therefore it's used a lot. \n",
    "\n",
    "\n",
    "I'll go ahead and define a schema for this dataset. To do so, I'll make use of the various data types provided within the pyspark.sql.types module (a full list of the supported types and their properties can be found in the following link https://athena.explore-datascience.net/student/content/train-view/64/161/3131).\n",
    "\n",
    "Note that below I read timestamps in as Integer types, I'll later cast them to Timestamp types. I did this because the Spark reader does not perfectly cast date types at read time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1dc6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required StructTypes in order to create the struct fields and in turn the schema\n",
    "\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49c808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Timestamp', IntegerType(), True),\n",
    "    StructField('Open', DoubleType(), True),\n",
    "    StructField('High', DoubleType(), True),\n",
    "    StructField('Low', DoubleType(), True),\n",
    "    StructField('Close', DoubleType(), True),\n",
    "    StructField('Volume_(BTC)', DoubleType(), True),\n",
    "    StructField('Volume_(Currency)', DoubleType(), True),\n",
    "    StructField('Weighted_Price', DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74c5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_schema = spark.read.csv\\\n",
    "            ('/Users/daluxmba/Desktop/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', \\\n",
    "             header=True,\\\n",
    "            schema = schema,\\\n",
    "            multiLine = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc045172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e15d8",
   "metadata": {},
   "source": [
    "Awesome, it looks all the schema changes applied to the dataframe have now taken effect. There's still the timestamp column to deal with though.\n",
    "\n",
    "I will need to cast this field to the appropriate type. However, to do this I will first have to better understand how to manipulate data in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5e068",
   "metadata": {},
   "source": [
    "## Altering column data\n",
    "\n",
    "Manipulating the contents of a column, such as redefining its data type, is a common operation in Spark. To perform such a step, we call the `withColumn()` method on a DataFrame object. This method takes in two arguments:\n",
    "\n",
    "    The first is the colName, or \"column name\" argument, which is a string indicating the column we would like to manipulate. If we provide the name of an existing column within our DataFrame, the method will essentially update its contents. Alternatively, if we provide a new column name, the method will append this new column to the resulting DataFrame.\n",
    "    The second argument is the col, or \"column expression\", which defines what operation(s) should be performed to update/create the resulting column.\n",
    "\n",
    "In our instance, we want to update the existing Timestamp column; changing the data from an integer encoding to a more suitable timestamp format.\n",
    "\n",
    "To perform this conversion, we can use a powerful set of functions provided within PySpark's functions module. Here we are specifically interested in the to_timestamp() and to_date() functions, however, we'll soon see that we make extensive use of this module for many tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1516077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to import the sql functions module in order to perform the change and to apply functions to the dataframe.\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97137cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_time_corrected = btc_schema.withColumn(colName='Timestamp', col=F.to_timestamp(btc_schema[\"Timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc204cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_time_corrected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48eca4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add an extra column to the dataframe. The column will indicate the day of the week, Sunday = 1\n",
    "\n",
    "btc_time_corrected = btc_time_corrected.withColumn('Day', F.dayofweek(btc_time_corrected.Timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6360da50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|          Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Day|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|2011-12-31 09:52:00|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|  7|\n",
      "|2011-12-31 09:53:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|  7|\n",
      "|2011-12-31 09:54:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|  7|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_time_corrected.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ba97e",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87817310",
   "metadata": {},
   "source": [
    "Filerts and projections give us a way to select rows based on a condition we specify. In Pyspark projections are obtained using the `filter()` or the `where()` methods. They are basically the same and will produce the same result so it doesn't really matter qhich one you use.\n",
    "\n",
    "To put projections to the test, I'll filter out the dataframe to have data from 2019 onwards. To accompish this, I'll import the datetime and create a filter for the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "198e6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "filter_date = date(2019, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_2019 = btc_time_corrected.where(btc_time_corrected['Timestamp'] > filter_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f3236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+---+\n",
      "|          Timestamp|   Open|   High|    Low|  Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Day|\n",
      "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+---+\n",
      "|2019-01-01 00:01:00|3674.58|3674.68| 3663.8| 3663.8| 15.44882028|     56671.806953|  3668.3582258|  3|\n",
      "|2019-01-01 00:02:00|3664.41|3664.41|3664.41|3664.41|    0.015697|      57.52024377|       3664.41|  3|\n",
      "|2019-01-01 00:03:00|3666.13|3669.13|3662.85|3669.13| 27.15298682|     99532.493706|  3665.6186064|  3|\n",
      "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc_2019.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363f31e",
   "metadata": {},
   "source": [
    "Perfect, looks like the DF now starts from 2019 as apposed to 2011. So, from the above, we've just filtered based on the rows, now what if we want to select columns using spark? Well, we have the `select()` function for that purpose!\n",
    "\n",
    "Let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1894142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+\n",
      "|          Timestamp|   Open|  Close|\n",
      "+-------------------+-------+-------+\n",
      "|2019-01-01 00:01:00|3674.58| 3663.8|\n",
      "|2019-01-01 00:02:00|3664.41|3664.41|\n",
      "|2019-01-01 00:03:00|3666.13|3669.13|\n",
      "+-------------------+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc_2019.select('Timestamp','Open','Close').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c2586c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_date = date(2017,12,31)\n",
    "filter_day = F.dayofweek(btc_time_corrected[\"Timestamp\"]) == 2\n",
    "filter_close = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "304c7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|          Timestamp|  Close|\n",
      "+-------------------+-------+\n",
      "|2018-11-19 00:00:00|5541.61|\n",
      "|2018-11-19 00:01:00| 5541.8|\n",
      "|2018-11-19 00:02:00|5540.64|\n",
      "|2018-11-19 00:03:00|5540.64|\n",
      "+-------------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc_time_corrected.select('Timestamp','Close')\\\n",
    ".where((btc_time_corrected['Timestamp'] > filter_date) & (btc_time_corrected['Close'] < filter_close) &filter_day ).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74532e",
   "metadata": {},
   "source": [
    "## Adding, Renaming and Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd29177",
   "metadata": {},
   "source": [
    "The two main reasons for a DE to remane columns are **Readability** and **to maintain the naming convention**.\n",
    "\n",
    "Let's take a look at how I can improve both here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef55a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'Open',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Close',\n",
       " 'Volume_(BTC)',\n",
       " 'Volume_(Currency)',\n",
       " 'Weighted_Price',\n",
       " 'Day']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = btc_2019.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446916ad",
   "metadata": {},
   "source": [
    "In my personal opinion, you can never go wrong with making making the column names small letters. Also I believe removing special characters like '()' is warrented and will improve readability. I also want to save this file as a parquet file therefore no spaces between the names. Lets get it.\n",
    "\n",
    "\n",
    "The one function I can use to rename a column is the `withColumnRenamed` function available in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33229a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = btc_time_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9889406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|          Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Day|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|2011-12-31 09:52:00|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|  7|\n",
      "|2011-12-31 09:53:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|  7|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5953be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in btc.columns:\n",
    "    btc = btc.withColumnRenamed(col,col.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c60232df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|          timestamp|open|high| low|close|volume_(btc)|volume_(currency)|weighted_price|day|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "|2011-12-31 09:52:00|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|  7|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18031465",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = btc.withColumnRenamed('volume_(btc)',\"volume_btc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "80870dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = btc.withColumnRenamed('volume_(currency)',\"volume_currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35a0dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+----------+---------------+--------------+---+\n",
      "|          timestamp|open|high| low|close|volume_btc|volume_currency|weighted_price|day|\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+---+\n",
      "|2011-12-31 09:52:00|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|  7|\n",
      "|2011-12-31 09:53:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|  7|\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf38670",
   "metadata": {},
   "source": [
    "I'm feeling pretty good with this result for now. I have read in csv data using spark, corrected the schema and have renamed the columns appropriately. Now I am ready to save the result as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "283c4cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc.write.format(\"parquet\").save(\"./transaformed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49785743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
